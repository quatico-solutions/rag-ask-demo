# =============================================================================
# Server Configuration
# ======  =======================================================================
PORT=8787

AI_COMPLETION_PROVIDER=lmstudio
AI_EMBEDDING_PROVIDER=lmstudio

# =============================================================================
# LMStudio Configuration
# =============================================================================

# Required when using LMStudio provider
# Default LMStudio server URL (adjust port if needed)
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# LMStudio models for completions (when AI_COMPLETION_PROVIDER=lmstudio)
# Use the exact model name as it appears in LMStudio
# Recommended: 7B-13B parameter models for good performance/quality balance
AI_COMPLETION_MODEL=llama-2-7b-chat
AI_COMPLETION_MODEL=llama-2-13b-chat
AI_COMPLETION_MODEL=mistral-7b-instruct
AI_COMPLETION_MODEL=codellama-7b-instruct
AI_COMPLETION_MODEL=zephyr-7b-beta

# LMStudio models for embeddings (when AI_EMBEDDING_PROVIDER=lmstudio)
# Recommended: Sentence transformer models for embeddings
AI_EMBEDDING_MODEL=all-MiniLM-L6-v2
AI_EMBEDDING_MODEL=all-mpnet-base-v2
AI_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
AI_EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2


# =============================================================================
# Performance Notes
# =============================================================================

# For datasets with 100+ documents (e.g., pull request descriptions):
#
# LMStudio Recommendations:
# - Completions: 7B-13B parameter models (llama-2-7b-chat, mistral-7b-instruct)
# - Embeddings: all-MiniLM-L6-v2 (fast) or all-mpnet-base-v2 (higher quality)
# - Ensure sufficient VRAM (8-16GB for 7B models, 16-24GB for 13B models)
# - Use GPU acceleration for best performance
